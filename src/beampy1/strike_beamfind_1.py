# -*- coding: utf-8 -*-
"""Strike_beamfind_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bz_LHvu5gXsPuItwTnAuXsiE-bQ_3ICG
"""

#!/usr/bin/env python
# -*- coding: utf-8 -*-


from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
from sklearn import datasets
from sklearn import metrics
from sklearn import model_selection
from sklearn import preprocessing

import tensorflow as tf

# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.patches as patches


SMALL_SIZE = 2
MEDIUM_SIZE = 4
BIGGER_SIZE = 6

plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

plt.rcParams['figure.figsize'] = [10, 10]
plt.rcParams['figure.dpi'] = 200


sess = tf.InteractiveSession()

class Strike_ImageTools :
    
    # Sigma -> Full Width at Half Maximum
    @staticmethod
    def sigma2fwhm(sigma):
        return sigma * np.sqrt(8 * np.log(2))

    # Full Width at Half Maximum -> Sigma
    @staticmethod
    def fwhm2sigma(fwhm):
        return fwhm / np.sqrt(8 * np.log(2))

    # def gaussian_kernel(size: int, mean: float, std: float ):
    @staticmethod
    def tf_gsk(size, xy, std ):
        """Makes 2D gaussian Kernel for convolution."""
        x,y = xy
        size_x, size_y = size/2
        dx = tf.distributions.Normal(x, std)
        dy = tf.distributions.Normal(y, std)
        valsx = dx.prob(tf.range(start = -size_x, limit = size_x + 1, dtype = tf.float32))
        valsy = dy.prob(tf.range(start = -size_y, limit = size_y + 1, dtype = tf.float32))
        gauss_kernel = tf.einsum('i,j->ij', valsy, valsx)
        return gauss_kernel / tf.reduce_sum(gauss_kernel)

    @staticmethod
    def np_gsk(size, xy, std):
        def gaussian(x, mu, sig):
            return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))
        x,y = xy
        sx,sy = size/2
        valx = gaussian(np.linspace(-sx,sx,2*sx),x,std)
        valy = gaussian(np.linspace(-sy,sy,2*sy),y,std)
        kernel = np.einsum('i,j->ij', valx, valy)
        return kernel

    @staticmethod
    def show_image(img):
        plt.imshow(img, cmap="Greys")
        plt.show()
        plt.xlabel(" SHAPE = %s" % tf.shape(img) )

    @staticmethod
    def show_images(img, xys):
        dim = len(img)
        sqd = int(np.sqrt(dim))
        sqx = sqd + (dim - sqd*sqd > 0 )
        sqy = sqd + (dim - sqd*sqd > sqd )
        extent = [-45,45,-60,60]
        fig, ax = plt.subplots(sqx,sqy)        
        for i in range(dim):
            axi = ax[i%sqx, int(i/sqx)]
            axi.grid(False)
            axi.imshow(img[i], cmap="Greys", extent=extent)            
            x, y, s = xys[i]
            rect   = patches.Rectangle((y-1,-x-1),2,2, linewidth=1,edgecolor='r',fill=False)
            circle = patches.Circle((y,-x),s, fill=False, edgecolor="b")
            axi.add_patch(rect)
            axi.add_patch(circle)
        plt.draw()
        return fig

#@title Generation Parameters

# DATASET WITH MAP
#
# training_dataset = tf.data.Dataset.range(10).map(
#     # lambda x: gen_random_gaussian(500,x,100)
#     # lambda x: tf.random_uniform([2],1,10,tf.float32)
#     lambda x: gaussian_spherical_kernel(500, tf.random_normal([2],-500,500,tf.float32), 100)
#     ).repeat()


# DATASET WITH GENERATOR
IMAGE_SIZE = np.array([120,90])

dispersion_sigma_attenuation = 5 #@param {type:"slider", min:4, max:10, step:0.1}
gauss_shape_mean = 16            #@param {type:"number"}
gauss_shape_var  = 3             #@param {type:"number"}

def generator():
    import itertools
    for i in itertools.count(1):
        xy  = np.random.normal(0,IMAGE_SIZE/dispersion_sigma_attenuation,2) 
        s   = np.random.normal(gauss_shape_mean, gauss_shape_var, 1)
        img = Strike_ImageTools.np_gsk(IMAGE_SIZE,xy,s)
        yield ((i,np.append(xy,s)), img)

ds = tf.data.Dataset.from_generator(
                generator = generator, 
                output_types = ((tf.int32,tf.float32), tf.float32),
                output_shapes = (((),3), tuple(IMAGE_SIZE))
                )

#@title Parameters
learning_rate = 0.0005 #@param {type:"slider", min:0, max:0.002, step:0.0001}
num_steps = 500 #@param {type:"slider", min:0, max:1000, step:50}
batch_size = 100 #@param ["16", "36", "64", "100"] {type:"raw"}
display_step = 50 #@param {type:"integer"}
GAIN = 2 #@param {type:"integer"}

# Network Parameters
# n_input = 200 #@param {type:"integer"}
n_classes = 3
dropout = 0.25 #@param {type:"slider", min:0, max:1, step:0.01}

# Automatically refill the data queue when empty
# ds = ds.repeat()
# Create batches of data
ds = ds.batch(batch_size)
# Prefetch data for faster consumption
ds = ds.prefetch(batch_size)


iterator = ds.make_initializable_iterator()
sess.run(iterator.initializer)
(id,xys), X = iterator.get_next()

# # show first image from batch
# (id,xys), X = sess.run(next_value)
# Strike_ImageTools.show_image(X[0])

## SHOW INSTANCE EXAMPLE ...
# (id,xys), X = sess.run(iterator.get_next())
# Strike_ImageTools.show_images(X, xys).suptitle("TRAINING")
# plt.show()
# exit()

# -----------------------------------------------
# THIS IS A CLASSIC CNN (see examples, section 3)
# -----------------------------------------------
# Create model
def conv_net(x, n_classes, dropout, reuse, is_training):
    # Define a scope for reusing the variables
    with tf.variable_scope('ConvNet', reuse=reuse):
        # MNIST data input is a 1-D vector of  784 features (28*28 pixels)
        # Reshape to match picture format [Height x Width x Channel]
        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]
        imgX, imgY = IMAGE_SIZE
        x = tf.reshape(x, shape=[-1, imgY, imgX, 1])

        # Convolution Layer 
        conv = tf.layers.conv2d(x, 32, 10, activation=tf.nn.relu, strides=2)
        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2
        # conv = tf.layers.max_pooling2d(conv, 2, 2)

        # Convolution Layer 
        conv = tf.layers.conv2d(conv, 32, 10, activation=tf.nn.relu, strides=2)
        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2
        # conv = tf.layers.max_pooling2d(conv, 2, 2)        

        # Convolution Layer 
        conv = tf.layers.conv2d(conv, 32, 10, activation=tf.nn.relu, strides=2)
        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2
        # conv = tf.layers.max_pooling2d(conv, 2, 2)        

        # Flatten the data to a 1-D vector for the fully connected layer
        fc = tf.contrib.layers.flatten(conv)

        # Fully connected layer
        fc = tf.layers.dense(fc, 1024)
        fc = tf.layers.dropout(fc, rate=dropout, training=is_training)        

        # Fully connected layer
        fc = tf.layers.dense(fc, 1024)
        fc = tf.layers.dropout(fc, rate=dropout, training=is_training)        

        # Fully connected layer
        fc = tf.layers.dense(fc, 500)
        fc = tf.layers.dropout(fc, rate=dropout, training=is_training)        

        # Output layer, class prediction
        out = tf.layers.dense(fc, n_classes)
        # Because 'softmax_cross_entropy_with_logits' already apply softmax,
        # we only apply softmax to testing network
        # out = tf.nn.softmax(out) if not is_training else out
    return out

# Create a graph for training
logits_train = conv_net(X, n_classes, dropout, reuse=False, is_training=True)

# Create another graph for testing that reuse the same weights, but has
# different behavior for 'dropout' (not applied).
logits_test = conv_net(X, n_classes, dropout, reuse=True, is_training=False)

# Define loss and optimizer (with train logits, for dropout to take effect)
loss_op = GAIN * tf.reduce_mean(tf.losses.mean_squared_error(xys,logits_train))
# loss_op = GAIN * tf.reduce_prod(tf.losses.mean_squared_error(xys,logits_train))

optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
train_op  = optimizer.minimize(loss_op)

# Evaluate model (with test logits, for dropout to be disabled)
# correct_pred = tf.equal(tf.argmax(logits_test, 1), tf.argmax(xy, 1))
# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
accuracy = tf.reduce_mean(tf.losses.mean_squared_error(xys,logits_test))
# accuracy = GAIN * tf.reduce_prod(tf.losses.mean_squared_error(xys,logits_test)) 

# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()
sess.run(init)

# Training cycle
for step in range(1, num_steps + 1):

    # Run optimization
    sess.run(train_op)

    if step % display_step == 0 or step == 1:
        # Calculate batch loss and accuracy
        # (note that this consume a new batch of data)
        loss, acc = sess.run([loss_op, accuracy])
        print("Step " + str(step) + ", Minibatch Loss= " + \
              "{:.6f}".format(loss) + ", Training Accuracy= " + \
              "{:.6f}".format(acc))

# (id,xys), X = sess.run(iterator.get_next())
(id_in, xys_in), X_in = sess.run(iterator.get_next())
sample_out = sess.run(conv_net(X_in, n_classes, dropout, reuse=True, is_training=False))


Strike_ImageTools.show_images(X_in,sample_out).suptitle("INSTANCE")


print("Optimization Finished!")
plt.show()